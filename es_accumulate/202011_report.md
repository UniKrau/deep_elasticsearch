# 1、写入elasticsearch慢解决方案

https://elasticsearch.cn/question/10967

rally,设置1个client（bulk 为3000）测试写入elasticsearch，速度为5000每秒（可能是数据比较大），查看服务器负载cpu ,IO并不高。

rally,设置10个client（bulk 为3000）测试写入elasticsearch，速度为30000+每秒（可能是数据比较大），查看服务器负载cpu ,大约用了70%。

请问一下，

1、在设置1个client下，还能优化ES的写入速度么？为啥发挥不出来ES的性能呢？

2、在设置10个client下，写入速度快了，服务器负载也提高了，说明了发挥了ES的性能，ES这个原理是什么？

【回复】
- client少的时候你可以适当的调大bulk的大小，多试几种bulk尺寸就能试出当前环境下最合适的bulk尺寸了。

- 当加大了bulk尺寸就变相的减少了数据传输的次数，减少IO时间。

- 还有性能也和你当前client的带宽有关的，如果10个client不在同一个服务器上可以考虑下这种情形。

- bulk 就是一次批量数据问题 太大 太小都不合适需要根据每天数据大小，进行调整测试。

- 有两种方法可以提供数据导入的速度：
1） 把 replicas 设置为0，这样避免在数据导入时复制数据。等数据导入完毕后，再修改 replicas 的数量为想要的数据

2）在默认的情况下 refresh_interval 为1s，如果你有大批量的数据，你可以把这个 refresh_interval 设置为 -1，等数据导入完毕后，再设置为默认值，或者自己想要的值，

# 2、elasticsearch 官网下载中的oss 版本是什么意思？

【medcl 回复】

https://elasticsearch.cn/question/10979

oss 就是不包括 xpack 的版本，你如果要带 xpack 的版本就下默认的版本就行了，并且也没有单独的 xpack 插件下载了哈，何必这样麻烦呢。

# 3、【非常精彩】图解 Elasticsearch 搜索底层原理

https://elasticsearch.cn/question/10976

# 4、Result window is too large 问题

从上面的报错信息，可以看到ES提示我结果窗口太大了，并且在后面也提到了要求我修改index.max_result_window参数来增大结果窗口大小。除了这个方法还有啥好的方法呢   也不能总是改这个设置

【回复1】

index.max_result_window 指定深度分页时最多查询多少条，默认是10000，你这设置的也太大了吧，不怕OOM吗

【回复2】
这个问题的解决源头在场景，想清楚你为啥需要这么深度的分页，是否有必要。如果是大量数据导出的场景用scroll

# 5、关于Elasticsearch 读取数据的方式【精彩讨论】

https://elasticsearch.cn/question/10989

# 6、term _id 查询导致磁盘读非常高

根据_id查询没有用GET，而是通过term去查询，结果导致磁盘非常高
 机器配置3台 16C/32G jvm16G

业务是按着月份分的索引，分别为index-2020.09， index-2020.10，index-2020.11
每个索引12个主分片
每个索引的大小都是100G，共300G

```
GET index-*/_search
{
"query": {
"term": {
"_id": {
"value": "2ec1d06bc8734447a0c71bc09e2a1845_592"
}
}
}
}
```
_id是自动生成的id

QPS非常低，只有2个左右，但是造成了磁盘的大量的读，读在150MB左右，磁盘负载很高

目前猜想，换成get会变好，想问下上面这个查询什么原理，造成这么大的读磁盘

# 7、elasticsearch拆分索引【方案探讨】

请教各位大佬一个问题，我们公司将每个应用的日志都存在一个索引里面，其中包含Error、Info、Debug等日志，每天会产生一个索引。

由于现在应用接入的越来越多，每天的索引量越来越大，我们现在想将单索引根据日志级别拆分为多个索引。

例如 error_202011_25、info_202011_25.。请问一下各位大佬有没有什么好的思路和解决方案

【回复1】

解决方案：
对存量数据处理：按日志级别新建索引，例：error_20201126_log、info_20201126_log。reindex旧索引数据到新索引的时候对日志级别进行过滤

对增量数据处理：写入es层进行改造。对日志级别做判断，写入不同的索引。

建议：按日志区分索引不合理，error日志量对比info日志量应该占很小比例，按日志级别拆效果不好。多个应用日志最好写入不同的日志索引里。如果需要跨多个索引搜索根据语法选择多个索引即可。

更多精彩：https://elasticsearch.cn/question/11025

# 8、logstash收取日志反向计算问题。

logstash在收取日志中有个PRI部分（一个数值），这部分是由facility和severity两部分组成的。方法是facility*8+severity。现在，在日志中得到一个数值，怎么在logstash中把severity部分计算出来？

比如：PRI=30=二进制0001 11100 其中serverity部分只最后三位 110=十进制6，我需要这个6。有什么方法可以把这个值在logstash中计算出来？
 
 
处理方法：
```
    ruby {

       code => "
            service = event.to_hash      #把event对象转换为hash对象
            a = service['syslog-num']   #读取日志的PRI值syslog-num
            b = Integer(a)                    #把PRI值转换成整数
            x = b.to_s(2)                      #把PRI值转换为二进制
            y = x[-3..-1]                       #读取二进制的最后三位，得到severity
            z = '0b' + y                        #把二进制值转化为机器认识的二进制码
            q = Integer(z)                     #把二进制转换为十进制
            event.set('syslog-num',q)    #把severity值写回syslog-num中
        "
    }
```
谢谢楼上和一只菜鸟的知乎分享。

# 9、es怎么获取最新数据？或者说取ID最大的一条数据怎么办？

数据已经超过一万条了，想像mysql那样取最新的一条数据，怎么搞都不对，网上查也无结果，有大佬知道吗？谢谢了!
我的代码是这样的：
```
GET /game/_search
{
"track_total_hits": true,
"query": {"range": {"gid": {"gte": 0 } } }, "size": 1
}
```

【回复】

通过pipeline在插数据的时候同时塞时间戳进去，取最新的一条就直接按时间戳倒排就好

ES应该没法直接查最新的记录，你可以对时间戳字段排序，取第一个；防止性能问题，你的查询条件最好别是null，不过你这总共才1w条应该没什么问题





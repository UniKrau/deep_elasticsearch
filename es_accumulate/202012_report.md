# 1、使用Elasticdump备份数据，发现备份的数据文档中_id错乱

使用Elasticdump备份集群A中的数据，备份命令如下：
```
./elasticdump --input=http:// ip:port/my_index --output=/data/my_index_data.json  --type=data
```
备份完后查看json文件中的数据发现每个文档中的_id都变了，并不是原始文档中的_id值，但是_source里面的内容是正确的。
注：索引中的数据的_id都是通过代码设置，并非随机值；

【medcl 回复】

elasticdump 不知道，esm 应该是没问题的。

https://github.com/medcl/esm 

# 2、怎么修改Elasticsearch的生命周期策略

问一下各位大佬，我们现在有一个需求是可以动态修改索引的生命周期策略。比如说，有一个索引将于10天后会被删除。

我可以修改这个生命周期策略，将10天删除改为30天删除。我看Kibana上只能删除重新添加，没有修改的地方。请问一下各位大佬有好的想法和思路么

【回复】

get _ilm/policy/xxx，修改策略后，再put就可以了。

# 3、【探讨】是否有设置可以保证主分片和副本分片都平均分配？

目前我设置total_shards_per_node保证分片均匀分配，但是这样只能保证每个节点上的分片数量相同，主副本分配还是随机的。

例如索引有12个分片，分配在3个节点。有的节点可能是3主1副分片，有的节点是1主3副分片。

这样在写入负载较高的索引上，实际运行时会导致比较明显的cpu资源不均衡，3主1副的节点cpu使用明显高于1主3副的。

即使我手动把分片调整为每个节点都是2主2副，也经常会因为自动均衡或者偶尔有主副分片切换，导致重新不均衡。
 
有没有什么分片分配的设置，能让ES自动保证主分片和副本分片都要保持平均呢？

【回复讨论】

我这边有两个情况:
1.集群里其他按天建的索引，在创建和删除时，会导致各节点分片略有不均，触发自动均衡，这时那个负载比较高的索引中的分片有可能就会迁移，这个通过total_shards_per_node可以限制住。

2.这个负载高的索引，本身会因为写入负载比较高，导致有时主分片像是挂掉一样，触发主副本切换，导致原来已经手动调整好的状态又被打破，这时又需要再进行手动调整，我遇到的这种情况比较多，没办法自动调整成上面说的主、副本分别都平均的状态。

# 4、是否可以在ES的search slow log中添加额外信息？

版本：ES-5.2.2

在排查问题时，通过search slow log虽然能找到具体的查询，但是很难找到对应的具体业务点。

所以想能不能在search的时候，传给ES服务端一个类似业务上的traceId，然后在慢查询中记录这个traceId，有助于排查这个慢查询是从哪里来的。

【回复】

慢日志打印时会输出你的query信息，你在查询时添加一个不存在的field，放上你的traceId，在输出日志时不就能看到traceId信息了吗

# 5、Elasticsearch主备集群如何实现数据的一致性

设计了Elasticsearch主备集群，数据会对两个集群进行双写，如何保证两个集群的数据一致性呢？是只要主集群写入成功就算成功，还是备集群数据写入成功才算成功呢？linux系统centos7.6，es7.6

【回复】

为啥要在应用层双写呢？直接用ES的工具CCR进行主备同步不好吗，不需要考虑一致性的问题，ES自身保证最终一致性。

CCR的主备限制有点多，follow不能直接转leader，leader集群挂了之后操作的步骤比较多，而且等leader 集群重新起来后又会有问题...另一种双向的follow也有很多限制....貌似没有什么完美的解决方案～

【回复2】

不用ccr的话，可以通过kafka双写，利用kafka的重试保证一致性

# 6、请教各位大佬，怎么查看索引处于ILM索引生命周期中的那个阶段

```
GET index/_ilm/explain
```

# 7、es 跨集群增量同步数据

es 7.10 ，快照可以完成两个集群数据的最终同步吗？本地集群和远程集群都会写入各自新的数据。不要求实时同步。

【回复】

快照的本质是备份数据

您的需求：应该类似双写或者同步，考虑下：elastic_dump reindex 或者logstash 同步 或者 medcl 大佬的开源的esm等

es跨集群还有ccr功能

# 8、【有趣讨论】ES堆内存分配越大，入库性能反而变小，有没有大佬知道为啥？

物理内存：32G
硬盘：200G
CPU：8核
 
ES版本：6.8.13
 
单机版测试。
 
数据量：1200万条日志，共6.8G
 
使用esrally进行压测，bulk size 5000docs，5个client
 
分别给ES分配2G、4G、6G、8G、10G、12G、14G、16G堆内存，结果发现分配内存越大，入库吞吐量反而变小了
 
分配2G时性能最好

【回复】

单从heap的角度分析：heap并不是越大越好，越大GC时间越长，反而会影响到那段时间的吞吐量

可以参考下 https://www.elastic.co/cn/blog/a-heap-of-trouble
 
如果目的是增加ES吞吐量，有其他方式的，比如调整refresh、translog、副本、niofs等等，和heap相关行没有那么大，主要是依赖ES的性能与系统的优化逻辑

# 9、为什么elasticsearch中副本碎片恢复的索引阶段如此缓慢？

为什么elasticsearch中副本分片恢复的索引阶段如此缓慢？

数据节点重启后，发现副本分片长时间处于初始化状态。

对于_cat/recovery接口，发现它们大多是副本分片，处于索引阶段。

副本分片恢复是向下恢复还是增量恢复？如何适应增量拉动

ES的版本是7.7。

谢谢你的回答。

【回复】

如果数据量比较大，恢复慢是正常的吧。调大

node_concurrent_recoveries

这个试试

# 10、【升级问题】请教各位大佬，es如何从6.6.2平滑升级到6.8

请教一下各位大佬，本人小白菜一枚刚开始接触ES，如有冒犯请多包涵。公司准备将es从6.6.2升级到6.8。测试环境为单机环境，生产环境为集群模式。请问一下各位大佬有没有什么好的建议升级 测试环境和生产环境。生产环境前提是保证数据不丢失，不重启平滑升级。测试环境前提是保证数据不丢失，可重启。以下是我的见解。
单机环境：拍摄ES快照，保存当前数据。将实时数据存储于Redis中，重新安装es6.8将快照进行恢复，然后把Redis中的实时数据插入到新es中。

【回复】

建议你先多读几遍upgrade guie :   https://www.elastic.co/guide/en/elastic-stack/6.8/upgrading-elastic-stack.html
另外测试环境应该也配置多节点，这样可以在测试环境上模拟生产环境升级的过程

# 11、es向量余弦相似度计算可以设置阈值吗？比如相似度大于0.8的才返回。

文档太多，但是如果用size来限制返回数量又不合理，比如设置size=100，有可能里面只有前几个是有用的，其他都不相似，也有可能里面全是相似的，而且还漏了很多，所以觉得用size不合理。想通过相似度的阈值来筛选一下文档。

【回复】

可以获取第一个文档的相关性打分，然后将这个打分乘以0.8，使用min_socre指定为该值重新查询

https://www.elastic.co/guide/en/elasticsearch/reference/7.2/search-request-min-score.html

# 12、Elastic Search CCR增量同步

各位大佬，目前在调研使用CCR来实现异地复制。

从官方的介绍和使用中，已经了解到其follow索引的创建、自动跟随等高级特性。也了解到CCR的基础模型是由follow索引发起pull请求来进行数据同步。

想问下，CCR是如何触发增量同步的呢？follow索引的时效性是如何保证的？

【回复】

leader上会注册follow的Listener，当有集群状态变更或者数据修改时，会notify这个Listener，

具体的实现你可以看es的源码：org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator，比如他的updateAutoFollowers 方法就是告诉follower集群状态变更了，同步增量数据过去。

- 1 创建follow任务时，follower节点会先恢复远程索引的数据
- 2 恢复完成后会创建一个定时任务
- 3. 根据你的配置请求leader（有最大等待时间，有重试机制，具体看官方文档，一旦拿到新数据，会立刻进行下一次请求），获取上次的translog的seqNo到最新的（根据配置调整单次的获取量）的operations
- 4. 持久化leader的operations到本地。
- 5. 重复3-4

https://elasticsearch.cn/question/11109

# 13、Too Many Requests rest并发查询报错es_rejected_execution_exception

【回复】

慢查询过多，自然search线程池排队的任务过多。看你的查询语句，regexp和size过多的分桶聚合都是性能非常差的。想办法从业务场景上优化，尽量避免使用这类的查询。

# 14、ES首次查询速度很慢

es版本号6.8.1，一台物理机部署了五个节点，每个节点的jvm内存设置成了31G，物理机内存大小为512G，索引大小130G，12亿条数据，第一次查询时间几十秒，后面就是毫秒级了，

做了些预加载的设置（index.store.preload: ["nvd", "dvd", "tim", "doc", "dim"]），但好像不起作用，DSL语句：{


https://elasticsearch.cn/question/11132

【回复】

第一次要把那些用到的文件加载到文件缓存里，这样下次用到就能直接从缓存里读取了，ES的数据很大部分是不加载到JVM堆里的，所以第一次慢以后快是正常的

聚合数据的话, 可以用到分片缓存， 注意使用

官网URL+/guide/en/elasticsearch/reference/current/shard-request-cache.html


可以试试用execution_hint，把segment再merge一下

# 15、求每天日志量的平均值，

实现参考：
```
POST /_search
{
  "size": 0,
  "aggs": {
    "sales_per_month": {
      "date_histogram": {
        "field": "date",
        "calendar_interval": "month"
      },
      "aggs": {
        "sales": {
          "sum": {
            "field": "price"
          }
        }
      }
    },
    "avg_monthly_sales": {
      "avg_bucket": {
        "buckets_path": "sales_per_month>sales" 
      }
    }
  }
}
```

# 16、什么情况会集群uuid不一致导致加入集群一直失败

https://elasticsearch.cn/question/11141

【回复】

uuid是集群的唯一标识。节点之间 cluster state不一致就可能出现，比如把一个节点从A集群移动到B集群，或者不小心误删了一个节点的cluster state。
 
你这个问题可以考虑复位cluster uuid，看下elasticsearch-node detach-cluster命令








# 1、网站报错，elastic索引莫名其妙的被关闭，需要重新开启索引？
暴露公网，被关闭索引
第二天上班发现网站打不开了，使用elasticsearch-head查看其他的索引变成index:close，莫名其妙的被关闭了；需要重新开启索引才行。而且生成了一个索引名：a_close_your_elastic_port9200_then_opentheindexagain；
我有二个centos服务器都是这种情况，就隔几分钟几乎同时发生的。是配置的问题还是被别人黑了 还是怎么了 奇了个怪？
https://elasticsearch.cn/question/10020

原因：看样子是被人为的关闭的，你是不是把 es 暴露在公网了，有很多脚本会自动扫描

# 2、esrally官网压测的数据，在内网有吗？
Q：esrally 是一个压测工具，我们可以下载下来自行压测自己的集群，当然官方也有基于 esrally 的 benchmark 持续更新。详细参考：https://elasticsearch-benchmarks.elastic.co/

# 3、es 放到 data 目录里的 文档json 没压缩吧？
A：底层 lucene 的数据结构分为多种，行存原始 json 在 store field 中是有压缩的，支持不同的压缩算法，可以配置，默认是LZ4，通过 best_compress 参数可以选择压缩比更高的 deflate，但性能也有些影响。列存的 doc value 有非常类似 RLE 的编码策略压缩，但是做的不完善。我们在内核优化版本里面做了加强。

# 4、ES5以上版本可以设置flush的interval吗？
我看老版本的ES文档上是有 index.translog.flush_threshold_period 这个参数的，可以指定多久后进行一次flush操作，默认是30分钟。
 
但我在ES5以上版本试了下发现这个参数已经没有了，只能设置index.translog.flush_threshold_size参数，指定translog达到一定大小后才flush。
 
这样会出现一个问题，当我把flush_threshold_size设置的较大时，比如4gb，所有不再有写入的索引，它们的每个分片上都有0-4g不等大小的tranglog文件，这些文件加起来占用的磁盘空间很可观。我只能每天手动对全部索引做一次flush才能释放掉。
 
为什么会取消flush_threshold_period这个参数呢，如果有了这个参数，我上面说的情况就可以解决了。


回复：
从 5.0 版本开始，索引的设置，不再使用配置文件elasticsearch.yaml 或者命令行。

In previous versions Elasticsearch allowed to specify index level setting as defaults on the node level, inside the elasticsearch.yaml file or even via command-line parameters. From Elasticsearch 5.0 on only selected settings like for instance index.codec can be set on the node level. All other settings must be set on each individual index. To set default values on every index, index templates should be used instead.

可以使用动态索引设置
```
curl -XPUT 'localhost:9200/my_index/_settings' -d '
{
    "index" : {
        "translog" : {
            "flush_threshold_size" : "512mb"
        }
    }
}'
```

https://www.elastic.co/guide/en/elasticsearch/reference/5.0/index-modules-translog.html

# 5 、【经常被问】Kibana 中 beats监控数据如何计算得到的呢？
medcl 回复：这种 rates 吞吐，用 datehistgram aggs，取对应指标的 count 就出来了。

# 6、聚合查询是否支持类似SQL IN的用法
SELECT COUNT(*) FROM test_select WHERE transID IN (SELECT transID FROM test_select WHERE `msg` = "TransStart" AND `type` = "Login") AND `result` = "Succ"
 
在如上数据中执行以下SQL查询，直接查找出 result为Succ并且type=Login的数据，在ES中是否支持这种查询呢？

【medcl】：Elasticsearch 暂时还不支持 IN query。

# 7、关于 CrateDB 和 Elasticsearch
有人实际用过CrateDB吗？感觉基于Elasticsearch的GroupBy做了一些不同的调增，并支持join
附文：https://crate.io/cratedb-comparison/cratedb-vs-elasticsearch/

medcl 回复：join 慎用，如果一定要用，用关系型数据库。

# 8、【数据清洗】请问一下，用canal同步复杂数据到ES的问题
需要实现一个功能，增量或者全量同步数据到ES，但是映射的字段比较复杂，很多字段需要计算后得出的值才能映射过去，如果采用canal能实现这个功能吗，很多字段不是一对一映射，当某个字段改变了，需要经过复杂计算，然后把计算结果同步到ES中的某个字段

回复：一个参考，canal数据经过kafka 然后接spark stream 翻译后再接kafka ,如果数据处理简单，可以用ingest节点处理

# 9、es API 查询 和 数据类型的奇妙关系
碰到了一个小问题： es 版本为 7.6， text类型的数据 使用 wildcard 方法是无法查出来的，意识到这个问题很好解决。但是想学习一下关于es 数据结构比较深层次的知识，以便于再碰到这样的问题，我能明白是因为什么而查不出来。
求大神推荐较全的资料，非常感谢

回复：简单说下，
第一：es在5.x后分为两种数据类型：keyword和text
两种类型适用场景不同：
keyword适合精准匹配，举例：手机号字段
text适合基于分词的全文检索，举例：正文内容字段

第二：检索类型 wildcard类似mysql like语句，本质也是模糊匹配 不是全文检索
不同检索类型的选型 建议：kibana dev tool
对比测试下

【medcl】：wildcard是通配符匹配，要作用在一个完整的字符串上面。
keyword 类型的字段是一个整字符串term；
text 类型会分词成多个字符 term；
 
可以了解下 Elasticsearch 的 Mapping 和 Lucene 的原理 。
# 10、关于index.translog.durability的疑问
index.translog.durability有两个参数，request和async,我对这两个参数都不是很理解。
官方文档7.7版本：https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-translog.html#
request的解释是
(default) fsync and commit after every request. In the event of hardware failure, all acknowledged writes will already have been committed to disk.
这里是指什么意思啊？是指每个请求都执行一次fsync，可是不是说fsnyc代价大，要放到translog中吗？
然后第二个参数async解释是
fsync and commit in the background every sync_interval. In the event of a failure, all acknowledged writes since the last automatic commit will be discarded.
如果选择了这个参数后又会导致什么变化呢？
看了很久都没搞懂，来论坛问问大家，谢谢了。

回复：fsync：对应系统级别的中的direct I/O，对直接写入到磁盘中，不经过page cache
async：对应系统级别的buffer I/O，会写到page cache，在{index.translog.sync_interval}时间以后会把page cache中的内容再写入到磁盘。比fsync快，但是在index.translog.sync_interval期间机器宕机的话，这些数据没写入到磁盘中，会丢失。

# 11、ElasticSearch 7.x 不需要优化分片的数量了吗？

每个分片上的数据量过大会导致查询过慢，因此当数据量过大时需要指定分片的数量来保证每个分片数据大小在30G以下。
es7.x版本之后不需要指定分片数量了吗？默认一个分片不会影响性能吗？本人才疏学浅，没在官方文档中找到答案。望大佬解答

【medcl】使用 ILM 自动进行切分很方便，可以限制单个分片固定大小比如 30G。

分片（shard）：一个ES的index由多个shard组成，每个shard承载index的一部分数据。

副本（replica）：index也可以设定副本数（numberofreplicas），也就是同一个shard有多少个备份。对于查询压力较大的index，可以考虑提高副本数（numberofreplicas），通过多个副本均摊查询压力。

shard数量（numberofshards）设置过多或过低都会引发一些问题：shard数量过多，则批量写入/查询请求被分割为过多的子写入/查询，导致该index的写入、查询拒绝率上升；对于数据量较大的inex，当其shard数量过小时，无法充分利用节点资源，造成机器资源利用率不高 或 不均衡，影响写入/查询的效率。

对于每个index的shard数量，可以根据数据总量、写入压力、节点数量等综合考量后设定，然后根据数据增长状态定期检测下shard数量是否合理。

基础架构部数据库团队的推荐方案是：

对于数据量较小（100GB以下）的index，往往写入压力查询压力相对较低，一般设置3~5个shard，numberofreplicas设置为1即可（也就是一主一从，共两副本） 。

对于数据量较大（100GB以上）的index：

一般把单个shard的数据量控制在（20GB~50GB）

让index压力分摊至多个节点：可通过index.routing.allocation.totalshardsper_node参数，强制限定一个节点上该index的shard数量，让shard尽量分配到不同节点上

综合考虑整个index的shard数量，如果shard数量（不包括副本）超过50个，就很可能引发拒绝率上升的问题，此时可考虑把该index拆分为多个独立的index，分摊数据量，同时配合routing使用，降低每个查询需要访问的shard数量。

# 12、对于纯写入节点是否有必要把index buffer设置很高呢？
indices.memory.index_buffer_size 这个值默认是10%，我测试过纯写入情况下，把这个值分别调成30% 50% 80%，发现数据写入速率只有轻微的提高。
 
我的一个es集群，由于经常有从其他数据源导入数据，导入完成后不再有写入，只有纯查询的情况，为了避免导入时影响查询，现在是专门设置了一组节点用于导数据，完成之后分片迁移到其他节点。
 
1.这组仅用于导数据的节点，有必要把index_buffer_size调大吗。除了这个参数外，还有其他参数需要调整吗。
2.假如调成了80%，是代表它会独占这80%内存，仅剩20%给其他用途，还是说内存是争用的，它最大能够用到80%呢。

回复：Q1:这个问题还挺有意思的，我尝试着回答下。首先index buffer这个池是用来缓冲数据写入的，数据写入buffer以后，默认1s以后就会被写入磁盘。所以除非你能在1s以内写入大于buffer池的内容（起码有几百M到几G），不然单纯的提升buffer池是没意义的。
再回到你的写入场景。你现在是要尽量的提升写的性能，可以在疯狂写入的期间（1）把集群rebalance关闭（2）把shard副本关闭，相信可以提升不少的写入性能。如果都做了，可以（1）降低下refresh的频率

（2）提高merge_factor的值，降低merge次数。但是我觉得可能意义不大。
最后，确保没有使用自定义id，bulk性能差非常多。
Q2:没记错是个最大值。

# 13、删除index下type的mapping

我搜索到的是可以删除某个type下的数据，那么还是留了mapping
我想把mapping也删除，请大佬指点，谢谢。
 
已知的删除数据语句如下：
```
POST index/type/_delete_by_query?conflicts=proceed
{
    "query":{
        "match_all":{}
    }
}
```
回复：mapping 建立完後只可以增加字段 不可以修改或刪除字段。
要修改mapping 用reindex 去處理，會比較好。
不然就是直接刪除整個index 重建。


